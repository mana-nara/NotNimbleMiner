# -*- coding: utf-8 -*-
"""NotNimbleMiner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X9woULFOFKjC0NOVitSIkT6z_9BgxW4u
"""

import os
import spacy
import numpy as np
import xml.etree.ElementTree as ET
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MultiLabelBinarizer

# Load spaCy language model
spacy.cli.download("en_core_web_lg")
nlp = spacy.load("en_core_web_lg")

# Constants and Configurations
xml_folder_path = '/content/drive/Shareddrives/ECS170: Group 2/NotNimbleMiner2/testing-PHI-noTags'
num_similar_terms = 5
word_window_width = 5
selected_terms_file = "selected_terms.txt"
embedding_model_file = "clinical_notes_embedding_model"

# # Define a custom tokenizer function with stop word removal, lowercase, and punctuation removal
# def custom_tokenizer(text):
#     # Use spaCy to tokenize the text
#     doc = nlp(text)
#     tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]
#     return tokens

#Mana's custom tokenizer
def custom_tokenizer(texts):
    tokenized_texts = []
    for text in texts:
        # Use spaCy to tokenize the text
        doc = nlp(text)
        tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]
        tokenized_texts.append(' '.join(tokens))  # Join tokens into a string
    return tokenized_texts

# # Stage 1: Tokenize clinical notes from XML files
# def tokenize_xml(xml_folder_path):
#     tokenized_corpus = []
#     for filename in os.listdir(xml_folder_path):
#         if filename.endswith(".xml"):
#             xml_path = os.path.join(xml_folder_path, filename)
#             tree = ET.parse(xml_path)
#             root = tree.getroot()
#             clinical_notes = " ".join(element.text for element in root.iter() if element.text)
#             tokens = custom_tokenizer(clinical_notes)
#             tokenized_corpus.append(tokens)
#     return tokenized_corpus

# Stage 1: Tokenize clinical notes from XML files
def tokenize_xml(xml_folder_path):
    raw = []
    for filename in os.listdir(xml_folder_path):
        if filename.endswith(".xml"):
            xml_path = os.path.join(xml_folder_path, filename)
            tree = ET.parse(xml_path)
            root = tree.getroot()
            clinical_notes = " ".join(element.text for element in root.iter() if element.text)
            raw.append(clinical_notes)
    return list(nlp.pipe(raw))

# Stage 2: Create word embedding model using only the training data
def create_word_embedding_model(tokenized_corpus, word_window_width):
    if os.path.exists(embedding_model_file):
        # Load the existing model if it exists
        embedding_model = Word2Vec.load(embedding_model_file)
    else:
        # Train a new model if it doesn't exist
        embedding_model = Word2Vec(tokenized_corpus, window=word_window_width, min_count=1, sg=0)
        # Save the Word2Vec model after training
        embedding_model.save(embedding_model_file)
    return embedding_model

# Stage 3: Explore vocabulary and select terms (based on user query)
def explore_vocabulary(embedding_model, num_similar_terms, max_approvals):
    selected_terms = []
    approvals = 0  # Initialize the approvals counter

    while approvals < max_approvals:
        user_query = input("Enter a query term (or 'q' to quit): ")
        if user_query.lower() == 'q':
            break

        similar_terms = embedding_model.wv.most_similar(user_query, topn=num_similar_terms)

        print(f"Similar terms for '{user_query}':")
        for term, score in similar_terms:
            print(f"{term} (Similarity Score: {score})")

        user_input = input("Select a similar term or press 'Enter' to skip: ")
        if user_input != '':  # Check if the user selected a term
            selected_terms.extend(term.strip() for term in user_input.split(','))  # Extend the list with multiple terms
            approvals += 1
        else:
            print("Skipping term selection.")

    with open(selected_terms_file, "w") as f:
        for term in selected_terms:
            f.write(f"{term}\n")

    return selected_terms

# # Stage 4: Assign labels and review (automated labeling)
# def assign_labels_and_review(tokenized_corpus, selected_terms):
#     labeled_data = []

#     for note_tokens in tokenized_corpus:
#         assigned_labels = [term for term in selected_terms if term in note_tokens]
#         labeled_data.append(assigned_labels)

#     return labeled_data

#Stage 4, Mana
def assign_labels_and_review(tokenized_corpus, selected_terms):
    labeled_data = []

    for note_tokens in tokenized_corpus:
        assigned_labels = [term for term in selected_terms if term in [token.text for token in note_tokens]]
        labeled_data.append(assigned_labels)

    return labeled_data

# Set the maximum number of approvals
max_approvals = 5

# Tokenize clinical notes from XML files
docs = tokenize_xml(xml_folder_path)

from google.colab import drive
drive.mount('/content/drive')

for i,token in enumerate(docs[0]):
  print(token.text, token.pos_)
  if i == 10:
    break

#docs[0][2],docs[0][2].vector

flat_tokenized_corpus = []

for doc in docs:
  for token in doc:
    flat_tokenized_corpus.append(token.text)

flat_tokenized_corpus[:10]

clean_corpus = []

for word in flat_tokenized_corpus:
    word = word.strip()
    if '\n' in word or '\t' in word or word == '' or word.replace(".", "").isnumeric() or '+' in word:
        continue
    clean_corpus.append(word)

tokenized_corpus = clean_corpus

clean_corpus[0:10]

# Create word embedding model
from gensim.models import Word2Vec
word2vec_model = Word2Vec(sentences=[clean_corpus], vector_size=100, window=5, min_count=1, sg=0)
word2vec_model.train([clean_corpus], total_examples=1, epochs=10)
word2vec_model.save("word2vec.model")

# Explore vocabulary and select terms
selected_terms = explore_vocabulary(word2vec_model, num_similar_terms, max_approvals)

selected_terms

# Split the tokenized corpus into training and testing sets
X_train, X_test = train_test_split(docs, test_size=0.2, random_state=42)

X_train[:10], X_test[:10]

#Combine X_train and labeled_test_data for TfidfVectorizer fitting
all_data = X_train + X_test

## ignoring for now

X_train_tokenized = custom_tokenizer(X_train)
X_test_tokenized = custom_tokenizer(X_test)

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train_tokenized)
X_test_counts = count_vect.transform(X_test_tokenized)

print("X_train_counts shape:", X_train_counts.shape)
print("X_test_counts shape:", X_test_counts.shape)

count_vect.vocabulary_.get(u'palpable')

# # Use TfidfVectorizer to transform the text data for all_data
# from sklearn.feature_extraction.text import TfidfTransformer

# tfidf_transformer = TfidfTransformer()
# X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
# X_train_tfidf.shape
# tfidf_vectorizer = TfidfVectorizer()
# tfidf_vectorizer.fit_transform([" ".join(tokens) for tokens in all_data])

# # Transform X_train using the same vectorizer
# X_train_tfidf = tfidf_vectorizer.transform([" ".join(tokens) for tokens in X_train])


# ### skip for now
# X_train_tfidf = tfidf_vectorizer.transform([" ".join(tokens) for tokens in X_train])
# X_train_tfidf.shape

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

# Assuming X_train_counts is the result of applying CountVectorizer
# Make sure you have imported and initialized CountVectorizer before this step.

# Initialize TfidfTransformer and fit to X_train_counts
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

# Initialize TfidfVectorizer and fit to all_data (after converting tokens to strings)
tfidf_vectorizer = TfidfVectorizer()
tfidf_vectorizer.fit([" ".join([token.text for token in tokens]) for tokens in all_data])

# Transform X_train using the same vectorizer
X_train_tfidf = tfidf_vectorizer.transform([" ".join([token.text for token in tokens]) for tokens in X_train])

# skip for now

# # Ensure X_train is not empty
# if not X_train:
#     print("Error: X_train is empty.")
# else:
#     # Check for empty token lists in X_train
#     non_empty_X_train = [tokens for tokens in X_train if tokens]
#     if len(non_empty_X_train) != len(X_train):
#         print(f"Warning: {len(X_train) - len(non_empty_X_train)} empty token lists found in X_train.")
#     X_train = non_empty_X_train  # Update X_train to exclude empty token lists

#     # Print the shape of X_train after removing empty token lists
#     print("Shape of X_train after cleaning:", X_train_tfidf.shape)

# from sklearn.preprocessing import MultiLabelBinarizer

# # Assuming X_train contains your data and selected_terms is defined properly
# # Make sure you have imported and initialized MultiLabelBinarizer before this step.

# mlb = MultiLabelBinarizer()

# # Assuming assign_labels_and_review is a function that assigns labels based on some criteria
# labels = assign_labels_and_review(X_train, selected_terms)

# binary_labels = mlb.fit_transform(labels)

from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.linear_model import LogisticRegression

mlb = MultiLabelBinarizer()

# Assuming assign_labels_and_review is a function that assigns labels based on some criteria
labels = assign_labels_and_review(X_train, selected_terms)

binary_labels = mlb.fit_transform(labels)

# Use ravel() to convert binary_labels to 1-dimensional array
binary_labels = binary_labels.ravel()

classifier = LogisticRegression(solver='liblinear')

# Use binary_labels for training
classifier.fit(X_train_tfidf, binary_labels)

from sklearn.preprocessing import MultiLabelBinarizer

# Assuming assign_labels_and_review is a function that assigns labels based on some criteria
labels = assign_labels_and_review(X_test, selected_terms)

# Assuming mlb is already defined and fitted on training labels
binary_labels = mlb.transform(labels)

# Predict labels for test data (assuming predicted_multi_labels is defined)
# If you haven't predicted them yet, you need to do so first
predicted_multi_labels = classifier.predict(X_test_tfidf)

# Print classification report
print(classification_report(binary_labels, predicted_multi_labels))

#X_train

#binary_labels

# # Fit the mlb object on the original labels
# mlb.fit(binary_labels)

# # Create and train the classifier (Logistic Regression)
# classifier = LogisticRegression(solver='liblinear')

# ### original
# # classifier.fit(X_train_tfidf, binary_labels)  # Use binary_labels, not binary_labels_flattened


# #### new
# classifier.fit(X_train, binary_labels)  # Use binary_labels, not binary_labels_flattened

# Transform test data using the same vectorizer
X_test_tfidf = tfidf_vectorizer.transform([" ".join([token.text for token in tokens]) for tokens in X_test])

# Predict labels for test data
predicted_labels = classifier.predict(X_test_tfidf)

# Convert predicted_labels to a 2D binary label format
predicted_labels_2d = predicted_labels.reshape(-1, 1)

# Inverse transform the binary labels to multi-label format using the same mlb object
predicted_multi_labels = mlb.inverse_transform(predicted_labels_2d)

# Flatten the binary_labels array
binary_labels_flattened = binary_labels.ravel()

# Print the shapes of X_train_tfidf and binary_labels_flattened
print("Shape of X_train_tfidf:", X_train_tfidf.shape)
print("Shape of binary_labels_flattened:", binary_labels_flattened.shape)

# Check the number of samples in both arrays
print("Number of samples in X_train_tfidf:", X_train_tfidf.shape[0])
print("Number of samples in binary_labels_flattened:", len(binary_labels_flattened))

# # Print classification report for multi-label classification
# print(classification_report(assign_labels_and_review(X_test, selected_terms), predicted_multi_labels))